# KevinHuang2025_AI
## - Studying for NVidia Orin Solution
## - For JetPack6.2 versopm. running on AGX Orin, Orin NX, and Orin Nano series
#### --- Study 'https://elinux.org/Jetson_Zoo'
#### --- Model Zoo â†’ Object Detection â†’ YOLO11 â†’ Quickstart
#### --- Study 'https://docs.ultralytics.com/guides/nvidia-jetson/#start-with-native-installation'

### # Setup Jetpack Environment
```bash
echo "ğŸ”„ Updating package lists..."
sudo apt update

echo "â¬†ï¸ Performing system upgrade..."
sudo apt dist-upgrade -y

echo "âš ï¸ System will reboot to apply upgrades. Please re-run the second script after reboot."
read -p "Press Enter to reboot..."
sudo reboot
```
#### --- After press "Enter" to do system reboot, then

```bash
echo "ğŸ§  Installing NVIDIA JetPack..."
sudo apt install -y nvidia-jetpack

echo "ğŸŒ Installing Chromium browser..."
sudo apt install -y chromium-browser

echo "âœ… All done!"

# æª¢æŸ¥æ˜¯å¦å·²ç¶“å­˜åœ¨ç›¸é—œè¡Œï¼Œé¿å…é‡è¤‡æ·»åŠ 
if ! grep -q 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64' ~/.bashrc; then
    echo "" >> ~/.bashrc
    echo "# cuda" >> ~/.bashrc
    echo "export LD_LIBRARY_PATH=/usr/local/cuda/lib64" >> ~/.bashrc
    echo "export PATH=\$PATH:/usr/local/cuda/bin" >> ~/.bashrc
    echo "âœ… CUDA è·¯å¾‘å·²å¯«å…¥ ~/.bashrc"
else
    echo "âš ï¸ CUDA ç’°å¢ƒè®Šæ•¸å·²å­˜åœ¨æ–¼ ~/.bashrcï¼Œç•¥éæ·»åŠ "
fi

# é‡æ–°è¼‰å…¥ .bashrc
source ~/.bashrc
echo "ğŸ”„ ~/.bashrc å·²é‡æ–°åŠ è¼‰"
```
___
### # Run JTOP
```bash
echo "ğŸ å®‰è£ python3-pip..."
sudo apt update
sudo apt install -y python3-pip

echo "ğŸ” pip3 ç‰ˆæœ¬ç¢ºèªï¼š"
pip3 -V

echo "ğŸ“¦ å®‰è£ jetson-stats..."
sudo pip3 install jetson-stats

echo "âœ… å®‰è£å®Œæˆï¼ä½ å¯ä»¥ç”¨ä»¥ä¸‹æŒ‡ä»¤æŸ¥çœ‹ Jetson ç‹€æ…‹ï¼š"
echo "ğŸ‘‰ sudo jtop"
```
#### --- After install related API, then execute
```bash
sudo jtop
```
___
### # Run CPU and GPU performance test
BSD 2-Clause License

Copyright (c) 2020, Ville Timonen
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


```bash
echo "ğŸš€ è¨­å®š Jetson ç‚ºæœ€é«˜æ•ˆèƒ½æ¨¡å¼..."
sudo nvpmodel -m 0
sudo jetson_clocks

echo "ğŸ› ï¸ ä¸‹è¼‰ä¸¦ç·¨è­¯ GPU å£“åŠ›æ¸¬è©¦å·¥å…·..."
git clone https://github.com/anseeto/jetson-gpu-burn.git
cd jetson-gpu-burn
make

echo "ğŸ”¥ å•Ÿå‹• CPU å£“åŠ›æ¸¬è©¦ (èƒŒæ™¯åŸ·è¡Œ)..."
sudo apt install -y stress
stress -c $(nproc) &

echo "â³ ç­‰å¾… 3 ç§’å¾ŒåŸ·è¡Œ GPU å£“åŠ›æ¸¬è©¦..."
sleep 3

echo "âš™ï¸ å•Ÿå‹• GPU å£“åŠ›æ¸¬è©¦ï¼ˆæŒçºŒ 1000 ç§’ï¼‰..."
sudo ./gpu_burn 1000

echo "âœ… æ¸¬è©¦å®Œæˆã€‚ä½ å¯ä»¥ä½¿ç”¨ 'htop' æˆ– 'jtop' è§€å¯Ÿè³‡æºä½¿ç”¨ç‹€æ³ã€‚"
```
___
### # Run yolo in USB camera
#### --- Setup environment
```bash
echo "ğŸ”„ æ›´æ–°å¥—ä»¶åˆ—è¡¨..."
sudo apt update

echo "ğŸ å®‰è£ pip..."
sudo apt install -y python3-pip

echo "ğŸ“¦ æ›´æ–° pip è‡ªèº«..."
pip install -U pip

echo "ğŸ§  å®‰è£ Ultralytics YOLOv8..."
pip install ultralytics

echo "ğŸ”¥ å®‰è£ PyTorch 2.5.0 for JetPack 6.1..."
pip install https://github.com/ultralytics/assets/releases/download/v0.0.0/torch-2.5.0a0+872d972e41.nv24.08-cp310-cp310-linux_aarch64.whl

echo "ğŸ–¼ï¸ å®‰è£ torchvision 0.20.0..."
pip install https://github.com/ultralytics/assets/releases/download/v0.0.0/torchvision-0.20.0a0+afc54f7-cp310-cp310-linux_aarch64.whl

echo "ğŸ”§ å®‰è£ cuSPARSELt ä»¥è§£æ±º torch ç›¸ä¾å•é¡Œ..."
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/arm64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install libcusparselt0 libcusparselt-dev

echo "âš™ï¸ å®‰è£ ONNX Runtime GPU 1.20.0 (Python 3.10)..."
pip install https://github.com/ultralytics/assets/releases/download/v0.0.0/onnxruntime_gpu-1.20.0-cp310-cp310-linux_aarch64.whl

echo "ğŸ”¢ å®‰è£ numpy 1.23.5ï¼ˆç‰¹å®šç›¸å®¹ç‰ˆæœ¬ï¼‰..."
pip install numpy==1.23.5

pip3 install pycuda

echo "âœ… YOLOv8 + PyTorch 2.5 + ONNX ç’°å¢ƒå®‰è£å®Œæˆï¼"
```

#### --- using only 1 USB camera in python3
#### --- make "1_USB_camera.py" file
```bash
gedit 1_USB_camera.py
```
#### --- copy following in "1_USB_camera.py" file 
```bash
from ultralytics import YOLO
import cv2
import time

# æ¨¡å‹æª”æ¡ˆ
models = {
    'Detect': YOLO("yolo11n.pt"),
    'Segment': YOLO("yolo11n-seg.pt"),
    'Pose': YOLO("yolo11n-pose.pt"),
    'Classify': YOLO("yolo11n-cls.pt"),
}

# æ”å½±æ©Ÿè¨­å®š
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640) #960
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) #540

# åˆå§‹åŒ–åƒæ•¸
prev_time = time.time()
fps = 0.0
mirror = True  # é¡åƒæ¨¡å¼

while cap.isOpened():
    success, frame = cap.read()
    if not success:
        break

    if mirror:
        frame = cv2.flip(frame, 1)

    start_time = time.time()

    results_imgs = []
    for name, model in models.items():
        # æ¨è«–
        results = model.predict(frame, verbose=False)
        # ç¹ªåœ–
        img = results[0].plot()

        # åœ¨å·¦ä¸Šè§’åŠ å…¥æ¨¡å‹åç¨±
        cv2.putText(img, f"{name}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
        results_imgs.append(img)

    # è¨ˆç®— FPS
    curr_time = time.time()
    instant_fps = 1.0 / (curr_time - prev_time)
    fps = 0.9 * fps + 0.1 * instant_fps
    prev_time = curr_time

    # åˆä½µå½±åƒï¼ˆä¸Šä¸‹æ’ï¼‰
    top_row = cv2.hconcat(results_imgs[:2])
    bottom_row = cv2.hconcat(results_imgs[2:])
    final_display = cv2.vconcat([top_row, bottom_row])

    # é¡¯ç¤º FPS
    fps_text = f"FPS: {fps:.2f} | Mirror: {'ON' if mirror else 'OFF'}"
    cv2.putText(final_display, fps_text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)

    # é¡¯ç¤ºç•«é¢
    cv2.imshow("YOLOv8 Multi-Model Inference", final_display)

    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
    elif key == ord('m'):
        mirror = not mirror
        print(f"[INFO] Mirror mode: {'ON' if mirror else 'OFF'}")

cap.release()
cv2.destroyAllWindows()

```
#### --- then execute
```bash
python3 1_USB_camera.py
```

#### --- using 3 USB camera in python3
#### --- make "3_USB_camera.py" file
```bash
gedit 3_USB_camera.py
```
#### --- copy following in "3_USB_camera.py" file 
```bash
from ultralytics import YOLO
import cv2
import time

# å„æ”å½±æ©Ÿå°æ‡‰æ¨¡å‹ï¼ˆDetectã€Segmentã€Poseï¼‰
camera_configs = [
    {"id": 0, "model_path": "yolo11n.pt", "model_name": "Detect"},
    {"id": 2, "model_path": "yolo11n-seg.pt", "model_name": "Segment"},
    {"id": 4, "model_path": "yolo11n-pose.pt", "model_name": "Pose"},
]

# åˆå§‹åŒ–æ”å½±æ©Ÿèˆ‡æ¨¡å‹
cameras = []
for cfg in camera_configs:
    cap = cv2.VideoCapture(cfg["id"])
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    model = YOLO(cfg["model_path"])
    cameras.append({
        "cap": cap,
        "model": model,
        "name": cfg["model_name"]
    })

# åˆå§‹åŒ–åƒæ•¸
prev_time = time.time()
fps = 0.0
mirror = True  # é è¨­é¡åƒé–‹å•Ÿ

while all([cam["cap"].isOpened() for cam in cameras]):
    frames = []
    for cam in cameras:
        success, frame = cam["cap"].read()
        if not success:
            frame = None
        elif mirror:
            frame = cv2.flip(frame, 1)
        frames.append(frame)

    # è·³å‡ºå›åœˆå¦‚æœæœ‰ä»»ä¸€æ”å½±æ©Ÿå¤±æ•—
    if any(f is None for f in frames):
        break

    # æ¨è«–èˆ‡ç•«åœ–
    plotted_frames = []
    for i, cam in enumerate(cameras):
        results = cam["model"].predict(frames[i], verbose=False)
        plotted = results[0].plot()
        plotted_frames.append(plotted)

    # FPS å¹³æ»‘è¨ˆç®—
    curr_time = time.time()
    instant_fps = 1.0 / (curr_time - prev_time)
    fps = 0.9 * fps + 0.1 * instant_fps
    prev_time = curr_time

    # é¡¯ç¤ºæ¯å€‹æ”å½±æ©Ÿçš„ç•«é¢èˆ‡ FPS
    for i, frame in enumerate(plotted_frames):
        fps_text = f"FPS: {fps:.2f} | Model: {cameras[i]['name']} | Mirror: {'ON' if mirror else 'OFF'}"
        (text_width, _), _ = cv2.getTextSize(fps_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)
        x = int((frame.shape[1] - text_width) / 2)
        y = 30
        cv2.putText(frame, fps_text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        cv2.imshow(f"Camera {i} - {cameras[i]['name']}", frame)

    # éµç›¤æ“ä½œ
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
    elif key == ord('m'):
        mirror = not mirror
        print(f"[INFO] Mirror mode: {'ON' if mirror else 'OFF'}")

# é‡‹æ”¾è³‡æº
for cam in cameras:
    cam["cap"].release()
cv2.destroyAllWindows()
```
#### --- then execute
```bash
python3 3_USB_camera.py
```
#### --- updat. 3 USB camera & performance(CPU/GPU/Temp/Memory/Power) in one window
```bash
import numpy as np
import os
import cv2
import time
import subprocess
import threading
from ultralytics import YOLO

# å„æ”å½±æ©Ÿå°æ‡‰æ¨¡å‹ï¼ˆDetectã€Segmentã€Poseï¼‰
camera_configs = [
    {"id": 0, "model_path": "yolo11n.pt", "model_name": "Detect"},
    {"id": 2, "model_path": "yolo11n-seg.pt", "model_name": "Segment"},
    {"id": 4, "model_path": "yolo11n-pose.pt", "model_name": "Pose"},
]

# åˆå§‹åŒ–æ”å½±æ©Ÿèˆ‡æ¨¡å‹
cameras = []
for cfg in camera_configs:
    cap = cv2.VideoCapture(cfg["id"])
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    model = YOLO(cfg["model_path"])
    cameras.append({
        "cap": cap,
        "model": model,
        "name": cfg["model_name"]
    })

# åˆå§‹åŒ–åƒæ•¸
prev_time = time.time()
fps = 0.0
mirror = True  # é è¨­é¡åƒé–‹å•Ÿ

canvas_width = 1280
canvas_height = 960
frame_width = canvas_width // 2
frame_height = canvas_height // 2

# ç›£æ§è³‡æ–™å„²å­˜èˆ‡é–
monitor_data = {
    "cpu": 0, 
    "gpu": 0, 
    "temp": 0.0,
    "ram": "0/0MB",
    "power": "0mW"
}

data_lock = threading.Lock()

def parse_tegrastats(line):
    try:
        cpu_part = line.split("CPU [")[1].split("]")[0]
        cpu_usages = [int(c.split("%@")[0]) for c in cpu_part.split(",") if "%@" in c]
        cpu_avg = sum(cpu_usages) / len(cpu_usages)

        gpu_part = line.split("GR3D_FREQ ")[1].split("%")[0]
        gpu_usage = int(gpu_part.strip())

        temp_part = line.split("gpu@")[1].split("C")[0]
        gpu_temp = float(temp_part.strip())

        ram_part = line.split("RAM ")[1].split("MB")[0]
        ram_usage = ram_part.strip() + "MB"

        power_part = line.split("VIN_SYS_5V0 ")[1].split("mW")[0]
        power = power_part.strip() + "mW"

        return cpu_avg, gpu_usage, gpu_temp, ram_usage, power
    except Exception:
        return 0, 0, 0.0, "0/0MB", "0mW"

def tegrastats_monitor():
    proc = subprocess.Popen(["/usr/bin/tegrastats"], stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)
    for line in proc.stdout:
        cpu, gpu, temp, ram, power = parse_tegrastats(line)
        with data_lock:
            monitor_data["cpu"] = cpu
            monitor_data["gpu"] = gpu
            monitor_data["temp"] = temp
            monitor_data["ram"] = ram
            monitor_data["power"] = power

# å•Ÿå‹•èƒŒæ™¯ç›£æ§åŸ·è¡Œç·’
threading.Thread(target=tegrastats_monitor, daemon=True).start()

# ä¸»è¿´åœˆ
while all([cam["cap"].isOpened() for cam in cameras]):
    frames = []
    for cam in cameras:
        success, frame = cam["cap"].read()
        if not success:
            frame = None
        elif mirror:
            frame = cv2.flip(frame, 1)
        frames.append(frame)

    if any(f is None for f in frames):
        break

    plotted_frames = []
    for i, cam in enumerate(cameras):
        results = cam["model"].predict(frames[i], verbose=False)
        plotted = results[0].plot()
        plotted = cv2.resize(plotted, (frame_width, frame_height))
        plotted_frames.append(plotted)

    curr_time = time.time()
    instant_fps = 1.0 / (curr_time - prev_time)
    fps = 0.9 * fps + 0.1 * instant_fps
    prev_time = curr_time

    main_canvas = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)
    positions = [(0, 0), (frame_width, 0), (0, frame_height)]

    for i, frame in enumerate(plotted_frames):
        fps_text = f"FPS: {fps:.2f} | Model: {cameras[i]['name']} | Mirror: {'ON' if mirror else 'OFF'}"
        (text_width, _), _ = cv2.getTextSize(fps_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)
        x_text = int((frame.shape[1] - text_width) / 2)
        y_text = 30
        cv2.putText(frame, fps_text, (x_text, y_text), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        x, y = positions[i]
        main_canvas[y:y+frame_height, x:x+frame_width] = frame

    # ç¹ªè£½å³ä¸‹è§’ç¡¬é«”ç›£æ§åœ–
    with data_lock:
        cpu = monitor_data["cpu"]
        gpu = monitor_data["gpu"]
        temp = monitor_data["temp"]

    monitor_img = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)
    bar_width = int((frame_width - 100) * 0.8)
    x0 = 50

    def draw_bar(img, label, value, y, color):
        max_bar = int(bar_width * (value / 100))
        cv2.putText(img, f"{label}: {value:.1f}%", (x0, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        cv2.rectangle(img, (x0, y), (x0 + bar_width, y + 20), (100, 100, 100), 2)
        cv2.rectangle(img, (x0, y), (x0 + max_bar, y + 20), color, -1)

    draw_bar(monitor_img, "CPU", cpu, 60, (0, 255, 255))
    draw_bar(monitor_img, "GPU", gpu, 120, (0, 128, 255))
    cv2.putText(monitor_img, f"Temp: {temp:.1f}C", (x0, 180), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    cv2.putText(monitor_img, f"RAM: {monitor_data['ram']}", (x0, 220), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
    cv2.putText(monitor_img, f"Power: {monitor_data['power']}", (x0, 260), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

    main_canvas[frame_height:frame_height*2, frame_width:canvas_width] = monitor_img
    cv2.imshow("Monitor:Detect-Segment-Pose", main_canvas)

    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
    elif key == ord('m'):
        mirror = not mirror
        print(f"[INFO] Mirror mode: {'ON' if mirror else 'OFF'}")

for cam in cameras:
    cam["cap"].release()
cv2.destroyAllWindows()
```
___
### # Calculate TOPS by yolo
#### --- Please refer "# Run yolo in USB camera" to setup environment
#### --- make "TOPS.py" file
```bash
gedit TOPS.py
```
#### --- copy following in "TOPS.py" file 
```bash
from ultralytics import YOLO
import numpy as np
import torch
import time

model = YOLO("yolo11n.pt")
#model = YOLO("yolo11n-seg.pt")
#model = YOLO("yolo11n-pose.pt")
#model = YOLO("yolo11n-cls.pt")
dummy_input = torch.rand(1, 3, 640, 640)  # æ¨¡å‹å¤§å°è¦–æƒ…æ³èª¿æ•´

# é ç†±
for _ in range(10):
    model(dummy_input)

# æ¸¬è©¦
start = time.time()
num_runs = 50
for _ in range(num_runs):
    model(dummy_input)
end = time.time()

avg_time = (end - start) / num_runs
fps = 1.0 / avg_time
gflops = 6.5
tops = gflops / avg_time

print(f"\nğŸ“Š Model: yolo11n.pt")
print(f"â±ï¸  Average Inference Time: {avg_time*1000:.2f} ms")
print(f"ğŸ¯ Estimated FPS: {fps:.2f}")
print(f"âš™ï¸  Estimated TOPS: {tops:.2f} TOPS\n")
```
#### --- then execute
```bash
python3 TOPS.py
```
___
### # LLM benchmark
#### --- setup LLM environment
```bash
sudo apt update
sudo apt install -y docker.io
```

#### --- directly copy from "https://www.jetson-ai-lab.com/benchmarks.html"
```bash
#!/usr/bin/env bash
#
# Llama benchmark with MLC. This script should be invoked from the host and will run 
# the MLC container with the commands to download, quantize, and benchmark the models.
# It will add its collected performance data to jetson-containers/data/benchmarks/mlc.csv 
#
# Set the HUGGINGFACE_TOKEN environment variable to your HuggingFace account token 
# that has been granted access to the Meta-Llama models.  You can run it like this:
#
#    HUGGINGFACE_TOKEN=hf_abc123 ./benchmark.sh meta-llama/Llama-2-7b-hf
#
# If a model is not specified, then the default set of models will be benchmarked.
# See the environment variables below and their defaults for model settings to change.
#
# These are the possible quantization methods that can be set like QUANTIZATION=q4f16_ft
#
#  (MLC 0.1.0) q4f16_0,q4f16_1,q4f16_2,q4f16_ft,q4f16_ft_group,q4f32_0,q4f32_1,q8f16_ft,q8f16_ft_group,q8f16_1
#  (MLC 0.1.1) q4f16_0,q4f16_1,q4f32_1,q4f16_2,q4f16_autoawq,q4f16_ft,e5m2_e5m2_f16
#
set -ex

: "${HUGGINGFACE_TOKEN:=SET_YOUR_HUGGINGFACE_TOKEN}"
: "${MLC_VERSION:=0.1.4}"

: "${QUANTIZATION:=q4f16_ft}"
: "${SKIP_QUANTIZATION:=no}"
: "${USE_SAFETENSORS:=yes}"

#: "${MAX_CONTEXT_LEN:=4096}"
: "${MAX_NUM_PROMPTS:=4}"
: "${CONV_TEMPLATE:=llama-2}"
: "${PROMPT:=/data/prompts/completion_16.json}"

: "${OUTPUT_CSV:=/data/benchmarks/mlc.csv}"


function benchmark() 
{
    local model_repo=$1
    local model_name=$(basename $model_repo)
    local model_root="/data/models/mlc/${MLC_VERSION}"
    
    local download_flags="--ignore-patterns='*.pth,*.bin'"

    if [ $USE_SAFETENSORS != "yes" ]; then
      download_flags="--skip-safetensors"
    fi
    
    if [ ${MLC_VERSION:4} -ge 4 ]; then
      if [ -n "$HF_USER" ]; then
        hf_user="$HF_USER"
      else
        if [ $QUANTIZATION = "q4f16_ft" ]; then
          hf_user="dusty-nv"
        else
          hf_user="mlc-ai"
        fi
      fi
      
      mkdir -p $(jetson-containers data)/models/mlc/cache || true ;
      
      run_cmd="\
        python3 benchmark.py \
          --model HF://${hf_user}/${model_name}-${QUANTIZATION}-MLC \
          --max-new-tokens 128 \
          --max-num-prompts 4 \
          --prompt $PROMPT \
          --save ${OUTPUT_CSV} "
        
      if [ -n "$MAX_CONTEXT_LEN" ]; then
        run_cmd="$run_cmd --max-context-len $MAX_CONTEXT_LEN"
      fi
      
      if [ -n "$PREFILL_CHUNK_SIZE" ]; then
        run_cmd="$run_cmd --prefill-chunk-size $PREFILL_CHUNK_SIZE"
      fi
      
      run_cmd="$run_cmd ; rm -rf /data/models/mlc/cache/* || true ; "
    else
      run_cmd="\
        if [ ! -d \${MODEL_REPO} ]; then \
            MODEL_REPO=\$(huggingface-downloader ${download_flags} \${MODEL_REPO}) ; \
        fi ; \
        bash test.sh $model_name \${MODEL_REPO} "
    fi
    
    jetson-containers run \
        -e HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN} \
        -e QUANTIZATION=${QUANTIZATION} \
        -e SKIP_QUANTIZATION=${SKIP_QUANTIZATION} \
        -e USE_SAFETENSORS=${USE_SAFETENSORS} \
        -e MAX_CONTEXT_LEN=${MAX_CONTEXT_LEN} \
        -e MAX_NUM_PROMPTS=${MAX_NUM_PROMPTS} \
        -e CONV_TEMPLATE=${CONV_TEMPLATE} \
        -e PROMPT=${PROMPT} \
        -e OUTPUT_CSV=${OUTPUT_CSV} \
        -e MODEL_REPO=${model_repo} \
        -e MODEL_ROOT=${model_root} \
        -v $(jetson-containers root)/packages/llm/mlc:/test \
        -w /test \
        dustynv/mlc:0.1.4-r36.4.2 /bin/bash -c "$run_cmd"
}
            
   
if [ "$#" -gt 0 ]; then
    benchmark "$@"
    exit 0 
fi


#MLC_VERSION="0.1.0" MAX_CONTEXT_LEN=4096 USE_SAFETENSORS=off benchmark "princeton-nlp/Sheared-LLaMA-1.3B"
#MLC_VERSION="0.1.0" MAX_CONTEXT_LEN=4096 USE_SAFETENSORS=off benchmark "princeton-nlp/Sheared-LLaMA-2.7B"

#MLC_VERSION="0.1.0" MAX_CONTEXT_LEN=4096 benchmark "meta-llama/Llama-2-7b-hf"
#MLC_VERSION="0.1.1" MAX_CONTEXT_LEN=8192 benchmark "meta-llama/Meta-Llama-3-8B"

benchmark "meta-llama/Llama-3.2-1B-Instruct"
benchmark "meta-llama/Llama-3.2-3B-Instruct"
benchmark "meta-llama/Llama-3.1-8B-Instruct"
benchmark "meta-llama/Llama-2-7b-chat-hf"

MAX_CONTEXT_LEN=4096 PREFILL_CHUNK_SIZE=4096 benchmark "Qwen/Qwen2.5-0.5B-Instruct"
MAX_CONTEXT_LEN=4096 PREFILL_CHUNK_SIZE=4096 benchmark "Qwen/Qwen2.5-1.5B-Instruct"
MAX_CONTEXT_LEN=2048 PREFILL_CHUNK_SIZE=1024 benchmark "Qwen/Qwen2.5-7B-Instruct"

QUANTIZATION="q4f16_1" benchmark "google/gemma-2-2b-it"
#QUANTIZATION="q4f16_1" benchmark "google/gemma-2-9b-it"

benchmark "microsoft/Phi-3.5-mini-instruct"

benchmark "HuggingFaceTB/SmolLM2-135M-Instruct"
benchmark "HuggingFaceTB/SmolLM2-360M-Instruct"
benchmark "HuggingFaceTB/SmolLM2-1.7B-Instruct"
```
#### --- make "benchmark.py" file
```bash
gedit benchmark.py
```
#### --- copy following in "benchmark.py" file 
```bash
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# è®€å–èˆ‡æ¸…ç†è³‡æ–™
df = pd.read_csv('/home/aopen/jetson-containers/data/benchmarks/mlc.csv')
df.columns = df.columns.str.strip()

df['model'] = df['model'].str.replace('HF://dusty-nv/', '', regex=False)
df['model'] = df['model'].str.replace('HF://mlc-ai/', '', regex=False)
df['model'] = df['model'].str.replace('-q4f16_ft-MLC', '', regex=False)
df['model'] = df['model'].str.replace('-q4f16_1-MLC', '', regex=False)

# è¨ˆç®—å¹³å‡ä¸¦ä¾è§£ç¢¼ç‡æ’åº
df_unique = df.groupby('model', as_index=False).mean(numeric_only=True)
df_unique = df_unique.sort_values(by='decode_rate', ascending=False)

models = df_unique['model']
n = len(models)
y_pos = np.arange(n)

fig, ax1 = plt.subplots(figsize=(12, n * 0.6))

# ä¸»è»¸ï¼šè§£ç¢¼ç‡ï¼ˆè—è‰² barï¼‰
decode_pos = y_pos - 0.2
ax1.barh(decode_pos, df_unique['decode_rate'], height=0.35, color='skyblue', label='Decode Rate (tokens/sec)')
ax1.set_xlabel('Decode Rate (tokens/sec)', color='blue')
ax1.tick_params(axis='x', labelcolor='blue')
ax1.set_yticks(y_pos)
ax1.set_yticklabels(models)
ax1.invert_yaxis()

# twin x è»¸ï¼šè¨˜æ†¶é«”ç”¨é‡ï¼ˆç´…è‰² barï¼‰
ax2 = ax1.twiny()
memory_pos = y_pos + 0.2
ax2.barh(memory_pos, df_unique['memory'], height=0.35, color='lightcoral', label='Memory Usage (MB)')
ax2.set_xlabel('Memory Usage (MB)', color='red')
ax2.tick_params(axis='x', labelcolor='red')

# æ•¸å€¼æ¨™ç±¤ï¼ˆè§£ç¢¼ç‡ï¼‰
for i, val in enumerate(df_unique['decode_rate']):
    ax1.text(val + 1, decode_pos[i], f'{val:.0f} tokens/sec', va='center', color='navy')

# æ•¸å€¼æ¨™ç±¤ï¼ˆè¨˜æ†¶é«”ï¼‰
for i, val in enumerate(df_unique['memory']):
    ax2.text(val + 1, memory_pos[i], f'{val:.0f} MB', va='center', color='darkred')

# æ¨™é¡Œèˆ‡æ ¼å¼
plt.title('Models comparison', pad=20)
ax1.grid(True, axis='x', linestyle='--', alpha=0.4)
ax2.grid(False)
plt.tight_layout()
plt.show()
```
#### --- then execute
```bash
python3 benchmark.py
```
___
### # NVIDIA Jetson AI Lab - benchmark
```bash
git clone https://github.com/dusty-nv/jetson-containers
bash jetson-containers/install.sh
bash jetson-containers/packages/llm/mlc/benchmark.sh
```
#### --- then execute
```bash
python3 benchmark.py
```
___
### # Fix Chromium Brower can not open isue
#### --- hotfix
```bash
snap download snapd --revision=24724
sudo snap ack snapd_24724.assert
sudo snap install snapd_24724.snap
sudo snap refresh --hold snapd
sudo reboot
```
